#!/usr/bin/env python3
"""
Simple Direct Test of Agent Orchestrator
Tests the real analysis functionality without MCP client complexity
"""

import asyncio
from mcp_agent_orchestrator import AgentOrchestrator, AgentType

async def test_direct_agent_functionality():
    """Direct test of agent orchestrator without MCP protocol overhead"""

    print("ğŸ§ª DIRECT AGENT FUNCTIONALITY TEST")
    print("=" * 50)

    # Create orchestrator instance
    orchestrator = AgentOrchestrator()

    # Test 1: Create specialized agents
    print("\n1ï¸âƒ£ Creating specialized agents...")

    agents = [
        orchestrator.create_agent("FabricInvestigator", AgentType.RESEARCHER,
                                  ["webpack_analysis", "fabric_investigation", "script_debugging"]),
        orchestrator.create_agent("RootCauseAnalyst", AgentType.ANALYST,
                                  ["technical_analysis", "dependency_mapping", "error_diagnosis"]),
        orchestrator.create_agent("SystemArchitect", AgentType.ARCHITECT,
                                  ["system_design", "integration_planning", "recovery_strategy"]),
        orchestrator.create_agent("JSSpecialist", AgentType.SPECIALIST,
                                  ["js_debugging", "browser_analysis", "performance_optimization"])
    ]

    for agent in agents:
        print(f"   âœ… Created: {agent.name} ({agent.type.value}) - ID: {agent.id}")

    print(f"\n   ğŸ“Š Total Agents: {len(orchestrator.agents)}")

    # Test 2: Orchestrate real fabric.js analysis
    print("\n2ï¸âƒ£ Orchestrating REAL fabric.js analysis...")

    fabric_analysis_task = """
    CRITICAL FABRIC.JS LOADING FAILURE ANALYSIS:

    Context: yprint_designtool WordPress plugin has fabric.js loading issues.

    Key Investigation Points:
    1. fabric-global-exposer.js file exists but not registered in PHP
    2. vendor.bundle.js contains fabric module but not globally exposed
    3. Canvas initialization fails with 'fabric is undefined'
    4. System shows 404 errors for phantom emergency-fabric-loader.js

    Required: Technical root cause analysis with evidence-based findings and precise fix recommendations.
    """

    task = orchestrator.orchestrate_task(fabric_analysis_task, priority="critical")
    print(f"   ğŸ¯ Task Created: {task.id}")
    print(f"   ğŸ‘¥ Assigned Agents: {len(task.assigned_agents)}")
    print(f"   âš¡ Status: {task.status.value}")

    # Test 3: Wait for task completion and get REAL results
    print("\n3ï¸âƒ£ Waiting for REAL analysis results...")

    max_wait = 5  # seconds
    for i in range(max_wait):
        await asyncio.sleep(1)

        current_task = orchestrator.tasks[task.id]
        print(f"   â±ï¸  [{i+1}s] Status: {current_task.status.value}")

        if current_task.status.value == "completed":
            print(f"   âœ… Analysis completed!")
            break

    # Test 4: Examine REAL analysis results
    print("\n4ï¸âƒ£ EXAMINING REAL ANALYSIS RESULTS...")

    final_task = orchestrator.tasks[task.id]

    if final_task.status.value == "completed" and final_task.results:
        results = final_task.results
        print(f"   ğŸ“‹ Analysis Type: {results.get('analysis_type')}")
        print(f"   ğŸ¯ Root Cause: {results.get('findings', {}).get('root_cause')}")
        print(f"   ğŸ“Š Confidence: {results.get('confidence_level')}")

        print("\nğŸ” DETAILED TECHNICAL FINDINGS:")
        findings = results.get('findings', {})

        if 'evidence' in findings:
            print("   ğŸ“ Evidence:")
            for i, evidence in enumerate(findings['evidence'], 1):
                print(f"      {i}. {evidence}")

        if 'technical_details' in findings:
            print("   ğŸ”§ Technical Details:")
            tech_details = findings['technical_details']
            for key, value in tech_details.items():
                print(f"      â€¢ {key}: {value}")

        if 'recommended_fix' in results:
            print("   ğŸ› ï¸  Recommended Fix:")
            fix_steps = results['recommended_fix']
            for step_key, step_value in fix_steps.items():
                print(f"      â€¢ {step_key}: {step_value}")

        print(f"\nâœ… ANALYSIS COMPLETED BY: {results.get('analyzed_by', 'Agent System')}")
        print(f"   ğŸ“… Timestamp: {results.get('analysis_timestamp')}")

        # Test 5: Agent Performance Metrics
        print("\n5ï¸âƒ£ AGENT PERFORMANCE METRICS:")
        for agent_id in final_task.assigned_agents:
            if agent_id in orchestrator.agents:
                agent = orchestrator.agents[agent_id]
                metrics = agent.performance_metrics
                print(f"   ğŸ¤– {agent.name}:")
                print(f"      â€¢ Tasks Completed: {metrics['tasks_completed']}")
                print(f"      â€¢ Success Rate: {metrics['success_rate']:.1%}")
                print(f"      â€¢ Avg Execution Time: {metrics['avg_execution_time_ms']:.2f}ms")

        print("\n" + "=" * 50)
        print("ğŸ‰ DIRECT AGENT TEST SUCCESSFUL!")
        print("âœ… Agents provide REAL technical analysis, not mock responses!")
        print("âœ… Evidence-based findings with specific technical details!")
        print("âœ… Actionable fix recommendations with high confidence!")

        return True

    else:
        print(f"   âŒ Task failed or incomplete: {final_task.status.value}")
        if final_task.results and 'error' in final_task.results:
            print(f"   ğŸš¨ Error: {final_task.results['error']}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_direct_agent_functionality())
    if success:
        print("\nğŸš€ READY TO DEPLOY FUNCTIONAL AGENT SYSTEM!")
    else:
        print("\nâŒ Agent system needs debugging.")